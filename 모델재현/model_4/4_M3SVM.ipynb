{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce0409be-c03c-4876-8325-214565059103",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "\n",
    "# Regularized_loss와 R_MLR 함수 정의\n",
    "def Regularized_loss(model, n, y_pred, y_true, p=4, lam=0.01):\n",
    "    classification_loss = -torch.mean(y_true * torch.log_softmax(y_pred, dim=1))\n",
    "    RG_loss = 1/n * torch.norm(model.weight.unsqueeze(1) - model.weight.unsqueeze(0), p=2, dim=2).pow(p).sum()\n",
    "    loss = classification_loss + lam * RG_loss\n",
    "    return loss\n",
    "\n",
    "def R_MLR(para):\n",
    "    path = f'./dataset/{para[\"data\"]}.mat'\n",
    "    X = scio.loadmat(path)['X']\n",
    "    y = scio.loadmat(path)['Y'].squeeze()\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    n, d = X.shape[0], X.shape[1]\n",
    "    num_class = len(np.unique(y))\n",
    "\n",
    "    if para[\"If_scale\"]:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "    y = y - 1\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_tensor, y_tensor, test_size=para[\"test_size\"], random_state=para[\"state\"])\n",
    "\n",
    "    y_train = torch.nn.functional.one_hot(torch.tensor(y_train))\n",
    "    y_test = torch.nn.functional.one_hot(torch.tensor(y_test))\n",
    "\n",
    "    # 모델 및 옵티마이저 정의\n",
    "    model = torch.nn.Linear(d, num_class)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=para[\"lr\"], weight_decay=para[\"weight_decay\"])\n",
    "\n",
    "    # 손실 값과 정확도 저장용 리스트 초기화\n",
    "    loss_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for epoch in range(para[\"num_epoch\"]):\n",
    "        y_pred = model(X_train)\n",
    "        loss = Regularized_loss(model, n, y_pred, y_train, para[\"p\"], para[\"lam\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            # print(f\"Epoch [{epoch+1}/{para['num_epoch']}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X_test)\n",
    "                correct = (torch.argmax(y_pred, dim=1) == torch.argmax(y_test, dim=1)).sum().item()\n",
    "                test_acc = correct / len(X_test)\n",
    "                # print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            test_acc_list.append(test_acc)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "    return test_acc_list[-1]\n",
    "    \n",
    "# 데이터 별 파라미터 설정 함수\n",
    "def set_parameters(data_name, para):\n",
    "    para[\"data\"] = data_name\n",
    "    if data_name == 'HHAR':\n",
    "        para.update({\"lr\": 0.02, \"lam\": 0.0005, \"p\": 4, \"If_scale\": True})\n",
    "    elif data_name == 'Cornell':\n",
    "        para.update({\"lr\": 0.1, \"lam\": 0.005, \"p\": 1, \"If_scale\": False})\n",
    "    elif data_name == 'USPS':\n",
    "        para.update({\"lr\": 0.01, \"lam\": 0.001, \"p\": 4, \"If_scale\": True})\n",
    "    elif data_name == 'ISOLET':\n",
    "        para.update({\"lr\": 0.001, \"lam\": 0.001, \"p\": 4, \"If_scale\": False})\n",
    "    elif data_name == 'ORL':\n",
    "        para.update({\"lr\": 0.01, \"lam\": 0.01, \"p\": 6, \"If_scale\": True})\n",
    "    elif data_name == 'Dermatology':\n",
    "        para.update({\"lr\": 0.01, \"lam\": 0.1, \"p\": 6, \"If_scale\": False})\n",
    "    elif data_name == 'Vehicle':\n",
    "        para.update({\"lr\": 0.05, \"lam\": 0.0001, \"p\": 4, \"If_scale\": True})\n",
    "    elif data_name == 'Glass':\n",
    "        para.update({\"lr\": 0.01, \"lam\": 0.0001, \"p\": 4, \"If_scale\": True})\n",
    "    return para\n",
    "default_para = {\n",
    "    \"data\": \"Vehicle\",\n",
    "    \"num_epoch\": 500,\n",
    "    \"lr\": 0.05,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"lam\": 0.0001,\n",
    "    \"p\": 4,\n",
    "    \"state\": 42,\n",
    "    \"If_scale\": True,\n",
    "    \"test_size\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3adcb9f-9875-4012-b51b-b5aa24066eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(827, 4134) (827,)\n",
      "Final Test Accuracy: 0.8675\n",
      "(1560, 617) (1560,)\n",
      "Final Test Accuracy: 0.9487\n",
      "(10299, 561) (10299,)\n",
      "Final Test Accuracy: 0.9801\n",
      "(9298, 256) (9298,)\n",
      "Final Test Accuracy: 0.9462\n",
      "(400, 1024) (400,)\n",
      "Final Test Accuracy: 0.9750\n",
      "(366, 34) (366,)\n",
      "Final Test Accuracy: 0.9865\n",
      "(846, 18) (846,)\n",
      "Final Test Accuracy: 0.8000\n",
      "(214, 9) (214,)\n",
      "Final Test Accuracy: 0.7442\n"
     ]
    }
   ],
   "source": [
    "data_list = ['Cornell', 'ISOLET','HHAR', 'USPS',  'ORL', 'Dermatology', 'Vehicle', 'Glass']\n",
    "\n",
    "# 결과 저장 리스트\n",
    "results = []\n",
    "\n",
    "# 데이터별로 파라미터 업데이트 및 결과 저장\n",
    "for data in data_list:\n",
    "    para = default_para.copy()\n",
    "    para = set_parameters(data, para)\n",
    "    result = R_MLR(para)\n",
    "    results.append({\"data\": data, \"result\": result})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c035b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          data    result\n",
      "0         HHAR  0.980583\n",
      "1      Cornell  0.867470\n",
      "2         USPS  0.947312\n",
      "3       ISOLET  0.945513\n",
      "4          ORL  0.975000\n",
      "5  Dermatology  0.986486\n",
      "6      Vehicle  0.800000\n",
      "7        Glass  0.744186\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a8a8e",
   "metadata": {},
   "source": [
    "## L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6a442-fd3f-4ff9-b20d-d04983a79a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Regularized_loss(model, n, y_pred, y_true, p=4, lam=0.01,l1_ratio=0.5):\n",
    "    classification_loss = -torch.mean(y_true * torch.log_softmax(y_pred, dim=1))\n",
    "    L2_loss = 1/n * torch.norm(model.weight.unsqueeze(1) - model.weight.unsqueeze(0), p=2, dim=2).pow(p).sum()\n",
    "    L1_loss = torch.norm(model.weight, p=1)\n",
    "    loss = classification_loss + lam * ((1 - l1_ratio) * L2_loss + l1_ratio * L1_loss)\n",
    "    return loss\n",
    "\n",
    "def R_MLR(para):\n",
    "    path = f'./dataset/{para[\"data\"]}.mat'\n",
    "    X = scio.loadmat(path)['X']\n",
    "    y = scio.loadmat(path)['Y'].squeeze()\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    n, d = X.shape[0], X.shape[1]\n",
    "    num_class = len(np.unique(y))\n",
    "\n",
    "    if para[\"If_scale\"]:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "    y = y - 1\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_tensor, y_tensor, test_size=para[\"test_size\"], random_state=para[\"state\"])\n",
    "\n",
    "    y_train = torch.nn.functional.one_hot(torch.tensor(y_train))\n",
    "    y_test = torch.nn.functional.one_hot(torch.tensor(y_test))\n",
    "\n",
    "    # 모델 및 옵티마이저 정의\n",
    "    model = torch.nn.Linear(d, num_class)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=para[\"lr\"], weight_decay=para[\"weight_decay\"])\n",
    "\n",
    "    # 손실 값과 정확도 저장용 리스트 초기화\n",
    "    loss_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for epoch in range(para[\"num_epoch\"]):\n",
    "        y_pred = model(X_train)\n",
    "        loss = Regularized_loss(model, n, y_pred, y_train, para[\"p\"], para[\"lam\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            # print(f\"Epoch [{epoch+1}/{para['num_epoch']}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X_test)\n",
    "                correct = (torch.argmax(y_pred, dim=1) == torch.argmax(y_test, dim=1)).sum().item()\n",
    "                test_acc = correct / len(X_test)\n",
    "                # print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            test_acc_list.append(test_acc)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "    return test_acc_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef82aaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(827, 4134) (827,)\n",
      "Final Test Accuracy: 0.8072\n",
      "(1560, 617) (1560,)\n",
      "Final Test Accuracy: 0.8141\n",
      "(10299, 561) (10299,)\n",
      "Final Test Accuracy: 0.9757\n",
      "(9298, 256) (9298,)\n",
      "Final Test Accuracy: 0.9258\n",
      "(400, 1024) (400,)\n",
      "Final Test Accuracy: 0.0375\n",
      "(366, 34) (366,)\n",
      "Final Test Accuracy: 0.6757\n",
      "(846, 18) (846,)\n",
      "Final Test Accuracy: 0.8059\n",
      "(214, 9) (214,)\n",
      "Final Test Accuracy: 0.7209\n",
      "          data    result\n",
      "0      Cornell  0.807229\n",
      "1       ISOLET  0.814103\n",
      "2         HHAR  0.975728\n",
      "3         USPS  0.925806\n",
      "4          ORL  0.037500\n",
      "5  Dermatology  0.675676\n",
      "6      Vehicle  0.805882\n",
      "7        Glass  0.720930\n"
     ]
    }
   ],
   "source": [
    "data_list = ['Cornell', 'ISOLET','HHAR', 'USPS',  'ORL', 'Dermatology', 'Vehicle', 'Glass']\n",
    "\n",
    "# 결과 저장 리스트\n",
    "results = []\n",
    "\n",
    "# 데이터별로 파라미터 업데이트 및 결과 저장\n",
    "for data in data_list:\n",
    "    para = default_para.copy()\n",
    "    para = set_parameters(data, para)\n",
    "    result = R_MLR(para)\n",
    "    results.append({\"data\": data, \"result\": result})\n",
    "\n",
    "print(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfefe12",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb3b4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Regularized_loss(model, n, y_pred, y_true, p=4, lam=0.01,l1_ratio=0.5):\n",
    "    classification_loss = -torch.mean(y_true * torch.log_softmax(y_pred, dim=1))\n",
    "    RG_loss = 1/n * torch.norm(model.weight.unsqueeze(1) - model.weight.unsqueeze(0), p=2, dim=2).pow(p).sum()\n",
    "    loss = classification_loss + lam * RG_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "# 다층 퍼셉트론(MLP) 모델 정의\n",
    "class MLPModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.5)  # 과적합 방지용\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_data(X, scale_type='standard'):\n",
    "    if scale_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scale_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "# R_MLR 함수 수정\n",
    "def R_MLR(para):\n",
    "    path = f'./dataset/{para[\"data\"]}.mat'\n",
    "    X = scio.loadmat(path)['X']\n",
    "    y = scio.loadmat(path)['Y'].squeeze()\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    n, d = X.shape[0], X.shape[1]\n",
    "    num_class = len(np.unique(y))\n",
    "\n",
    "    if para[\"If_scale\"]:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "    y = y - 1\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_tensor, y_tensor, test_size=para[\"test_size\"], random_state=para[\"state\"])\n",
    "\n",
    "    y_train = torch.nn.functional.one_hot(torch.tensor(y_train))\n",
    "    y_test = torch.nn.functional.one_hot(torch.tensor(y_test))\n",
    "\n",
    "    # 모델 및 옵티마이저 정의\n",
    "    model = torch.nn.Linear(d, num_class)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=para[\"lr\"], weight_decay=para[\"weight_decay\"])\n",
    "\n",
    "    # 조기 종료 설정\n",
    "    best_loss = float('inf')\n",
    "    patience = 10\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Save the loss function values on the training set and the accuracy on the test set\n",
    "    loss_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for epoch in range(para[\"num_epoch\"]):\n",
    "        model.train()\n",
    "        y_pred = model(X_train)\n",
    "        loss = Regularized_loss(model, n, y_pred, y_train, para[\"p\"], para[\"lam\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            # print(f\"Epoch [{epoch+1}/{para.num_epoch}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X_test)\n",
    "                correct = (torch.argmax(y_pred, dim=1) == torch.argmax(y_test, dim=1)).sum().item()\n",
    "                test_acc = correct / len(X_test)\n",
    "                # print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            test_acc_list.append(test_acc)\n",
    "\n",
    "    print(f\"Total Test Accuracy : {test_acc:.4f}\")\n",
    "    return test_acc_list[-1]\n",
    "\n",
    "    # epochs = np.arange(1, len(loss_list) + 1) * 5\n",
    "    # conver_plot(epochs, test_acc_list, loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1500f506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(827, 4134) (827,)\n",
      "Total Test Accuracy : 0.8735\n",
      "(1560, 617) (1560,)\n",
      "Total Test Accuracy : 0.9455\n",
      "(10299, 561) (10299,)\n",
      "Total Test Accuracy : 0.9806\n",
      "(9298, 256) (9298,)\n",
      "Total Test Accuracy : 0.9473\n",
      "(400, 1024) (400,)\n",
      "Total Test Accuracy : 0.9750\n",
      "(366, 34) (366,)\n",
      "Early stopping at epoch 490\n",
      "Total Test Accuracy : 0.9865\n",
      "(846, 18) (846,)\n",
      "Total Test Accuracy : 0.8000\n",
      "(214, 9) (214,)\n",
      "Total Test Accuracy : 0.7442\n",
      "          data    result\n",
      "0      Cornell  0.873494\n",
      "1       ISOLET  0.945513\n",
      "2         HHAR  0.980583\n",
      "3         USPS  0.947312\n",
      "4          ORL  0.975000\n",
      "5  Dermatology  0.986486\n",
      "6      Vehicle  0.800000\n",
      "7        Glass  0.744186\n"
     ]
    }
   ],
   "source": [
    "data_list = ['Cornell', 'ISOLET','HHAR', 'USPS',  'ORL', 'Dermatology', 'Vehicle', 'Glass']\n",
    "\n",
    "# 결과 저장 리스트\n",
    "results = []\n",
    "\n",
    "# 데이터별로 파라미터 업데이트 및 결과 저장\n",
    "for data in data_list:\n",
    "    para = default_para.copy()\n",
    "    para = set_parameters(data, para)\n",
    "    result = R_MLR(para)\n",
    "    results.append({\"data\": data, \"result\": result})\n",
    "\n",
    "print(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10acb9",
   "metadata": {},
   "source": [
    "## RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17a712a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import torch\n",
    "\n",
    "# RBF 커널 변환 함수\n",
    "def apply_rbf_kernel(X, gamma=0.1):\n",
    "    \"\"\"\n",
    "    RBF 커널을 적용한 데이터 변환\n",
    "    Args:\n",
    "        X: 입력 데이터 (numpy 배열 또는 텐서)\n",
    "        gamma: RBF 커널 파라미터 (1 / (2 * sigma^2))\n",
    "    Returns:\n",
    "        RBF 커널로 변환된 데이터\n",
    "    \"\"\"\n",
    "    X_rbf = rbf_kernel(X, gamma=gamma)\n",
    "    return X_rbf\n",
    "\n",
    "# R_MLR 함수 수정: RBF 커널 적용\n",
    "def R_MLR_rbf(para):\n",
    "    # 데이터 로드\n",
    "    path = f'./dataset/{para[\"data\"]}.mat'\n",
    "    X = scio.loadmat(path)['X']\n",
    "    y = scio.loadmat(path)['Y'].squeeze()\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    n, d = X.shape[0], X.shape[1]\n",
    "    num_class = len(np.unique(y))\n",
    "\n",
    "    # 데이터 스케일링\n",
    "    if para[\"If_scale\"]:\n",
    "        X = preprocess_data(X, scale_type=para[\"scale_type\"])\n",
    "\n",
    "    # 레이블 정규화\n",
    "    y = y - 1\n",
    "\n",
    "    # RBF 커널 적용\n",
    "    X = apply_rbf_kernel(X, gamma=para.get(\"gamma\", 0.01))  # gamma 값은 하이퍼파라미터\n",
    "\n",
    "    # 데이터를 PyTorch 텐서로 변환\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # 데이터 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_tensor, y_tensor, test_size=para[\"test_size\"], random_state=para[\"state\"]\n",
    "    )\n",
    "\n",
    "    # 모델 정의\n",
    "    model = MLPModel(X.shape[1], num_class)  # RBF 적용 후 특성 차원에 맞게 수정\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=para[\"lr\"], weight_decay=para[\"weight_decay\"])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "    # 조기 종료 초기화\n",
    "    best_loss = float('inf')\n",
    "    patience = 10\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # 손실 및 정확도 저장 리스트\n",
    "    loss_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for epoch in range(para[\"num_epoch\"]):\n",
    "        # 학습 모드\n",
    "        model.train()\n",
    "        y_pred = model(X_train)\n",
    "        loss = Regularized_loss(model, n, y_pred, torch.nn.functional.one_hot(y_train, num_classes=num_class), para[\"p\"], para[\"lam\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # 조기 종료 조건 체크\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            # print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            # 검증 모드\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X_test)\n",
    "                correct = (torch.argmax(y_pred, dim=1) == y_test).sum().item()\n",
    "                test_acc = correct / len(X_test)\n",
    "                # print(f\"Epoch [{epoch + 1}/{para['num_epoch']}], Loss: {loss.item():.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            test_acc_list.append(test_acc)\n",
    "\n",
    "    # 최종 결과 출력 및 반환\n",
    "    print(f\"Total Test Accuracy for {para['data']} with RBF Kernel: {test_acc:.4f}\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04e3327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(827, 4134) (827,)\n",
      "Final Test Accuracy: 0.8675\n",
      "(1560, 617) (1560,)\n",
      "Final Test Accuracy: 0.9487\n",
      "(10299, 561) (10299,)\n",
      "Final Test Accuracy: 0.9806\n",
      "(9298, 256) (9298,)\n",
      "Final Test Accuracy: 0.9468\n",
      "(400, 1024) (400,)\n",
      "Final Test Accuracy: 0.9750\n",
      "(366, 34) (366,)\n",
      "Final Test Accuracy: 0.9865\n",
      "(846, 18) (846,)\n",
      "Final Test Accuracy: 0.7941\n",
      "(214, 9) (214,)\n",
      "Final Test Accuracy: 0.7209\n",
      "          data    result\n",
      "0      Cornell  0.867470\n",
      "1       ISOLET  0.948718\n",
      "2         HHAR  0.980583\n",
      "3         USPS  0.946774\n",
      "4          ORL  0.975000\n",
      "5  Dermatology  0.986486\n",
      "6      Vehicle  0.794118\n",
      "7        Glass  0.720930\n"
     ]
    }
   ],
   "source": [
    "data_list = ['Cornell', 'ISOLET','HHAR', 'USPS',  'ORL', 'Dermatology', 'Vehicle', 'Glass']\n",
    "\n",
    "# 결과 저장 리스트\n",
    "results = []\n",
    "\n",
    "# 데이터별로 파라미터 업데이트 및 결과 저장\n",
    "for data in data_list:\n",
    "    para = default_para.copy()\n",
    "    para = set_parameters(data, para)\n",
    "    result = R_MLR(para)\n",
    "    results.append({\"data\": data, \"result\": result})\n",
    "\n",
    "print(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d17025",
   "metadata": {},
   "source": [
    "## others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc5b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff3d266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
